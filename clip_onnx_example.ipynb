{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lednik7/CLIP-ONNX/blob/main/examples/clip_onnx_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxPg_VvZuScV"
      },
      "source": [
        "## Restart colab session after installation\n",
        "Reload the session if something doesn't work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "al_QNjyFq6Jj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/Lednik7/CLIP-ONNX.git\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "42eeJz9lTdJ6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget -c -O CLIP.png https://github.com/openai/CLIP/blob/main/CLIP.png?raw=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuauIZIBSEUX",
        "outputId": "2c7c2bd9-90dd-4b1a-e98a-79e1f2218644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Oct 10 09:19:11 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 545.36                 Driver Version: 546.33       CUDA Version: 12.3     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3070 ...    On  | 00000000:01:00.0  On |                  N/A |\n",
            "| N/A   44C    P8              18W / 115W |   1432MiB /  8192MiB |     27%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A        26      G   /Xwayland                                 N/A      |\n",
            "|    0   N/A  N/A        38      G   /Xwayland                                 N/A      |\n",
            "|    0   N/A  N/A        45      G   /Xwayland                                 N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gqvxpdajRX5_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU\n"
          ]
        }
      ],
      "source": [
        "import onnxruntime\n",
        "print(onnxruntime.get_device())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "010k-ksVTjAu"
      },
      "source": [
        "## CPU inference mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdTz0IJWVBqE"
      },
      "source": [
        "### Torch CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9ROPwKYurOhP"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# onnx cannot work with cuda\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\", jit=False)\n",
        "\n",
        "# batch first\n",
        "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).cpu() # [1, 3, 224, 224]\n",
        "image_onnx = image.detach().cpu().numpy().astype(np.float32)\n",
        "\n",
        "# batch first\n",
        "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).cpu() # [3, 77]\n",
        "text_onnx = text.detach().cpu().numpy().astype(np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CrHQ8cYt8Cx",
        "outputId": "4d98f85d-4b02-4ae2-b18f-fb3c7a2d6caf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "162 ms ± 13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit model(image, text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2MriaVVG6Y"
      },
      "source": [
        "### CLIP-ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSeG9uAZrcph",
        "outputId": "8c394684-d78e-49f6-a60f-872485d5f650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLIP ONNX] Start convert visual model\n",
            "=========== Diagnostic Run torch.onnx.export version 2.1.0a0+fe05266 ===========\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "[CLIP ONNX] Start check visual model\n",
            "[CLIP ONNX] Start convert textual model\n",
            "=========== Diagnostic Run torch.onnx.export version 2.1.0a0+fe05266 ===========\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "[CLIP ONNX] Start check textual model\n",
            "[CLIP ONNX] Models converts successfully\n"
          ]
        }
      ],
      "source": [
        "from clip_onnx import clip_onnx, attention\n",
        "clip.model.ResidualAttentionBlock.attention = attention\n",
        "\n",
        "onnx_model = clip_onnx(model)\n",
        "onnx_model.convert2onnx(image, text, verbose=True)\n",
        "# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        "onnx_model.start_sessions(providers=[\"CPUExecutionProvider\"]) # cpu mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B15dr51UrvMh",
        "outputId": "7c5fbc64-61f5-4742-d5a1-24d123971515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "216 ms ± 9.78 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit onnx_model(image_onnx, text_onnx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahh_7CkTUb8y"
      },
      "source": [
        "## GPU inference mode\n",
        "Select a runtime GPU to continue:\n",
        "\n",
        "Click Runtime -> Change Runtime Type -> switch \"Harware accelerator\" to be GPU. Save it, and you maybe connect to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6M7yq7qceb5"
      },
      "source": [
        "### CLIP-ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6LtPSZhfUd_m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "  if not isinstance(provider_options, collections.abc.Sequence):\n"
          ]
        }
      ],
      "source": [
        "onnx_model.start_sessions(providers=[\"CUDAExecutionProvider\"]) # GPU mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE0VGt9sQwrf",
        "outputId": "6feb4701-7b7f-437e-dc2f-c95c504dbb89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['CPUExecutionProvider']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "onnx_model.visual_session.get_providers() # optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPUVzqmgcYas",
        "outputId": "3e7c1526-6e38-4982-ca36-eabfc95c2ab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "230 ms ± 17.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%timeit onnx_model(image_onnx, text_onnx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb58mrkbch2V"
      },
      "source": [
        "### Torch CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gidR99GOckyF"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\"\n",
        "# onnx cannot work with cuda\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "# batch first\n",
        "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device) # [1, 3, 224, 224]\n",
        "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device) # [3, 77]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpBrtjlOcwOC",
        "outputId": "56375401-18a0-499b-f29b-c6e2d4d07e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 loops, best of 5: 72.2 ms per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit model(image, text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "clip_onnx_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
